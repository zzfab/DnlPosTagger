batch_size: 4
epochs: 1
checkpoint: null  # to load for warm start
devices: 1    # number of gpu devices
accelerator: "cpu"
accumulate_grad_batches: 4
max_epochs: 10
log_every_n_steps: 10
min_epochs: 25  # do not employ early stopping before this
save_top_k_epochs: 10

train_step:
  name: "loss/train"

val_step:
  name: "loss/val"

optim:
  lr: 1e-4
  weight_decay: 1e-8
  amsgrad: False

scheduler:
  patience: 5   # number of intervals to wait before updating lr
  factor: 0.1   # quantity to update lr by, divides lr by factor
  min_lr: 1e-5  # does not update lr to be smaller than this
  verbose: True
  monitor: "loss/val"  # quantity to monitor to determine need for lr update
  interval: "epoch"   # epoch or step
  frequency: 5

early_stop:
  monitor: val_step.name   # quantity to monitor to determine need for early stopping
  min_delta: 0.03          # minmum change required to be considered as a change in monitored quantity
  patience: 10            # monitor for these amny epoochs before stopping early
  mode: "min"             # quantity should be minimized



profiler:
  name: "train_0.log"
